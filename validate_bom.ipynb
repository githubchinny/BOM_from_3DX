{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate \n",
    "Author: Mark Chinnock  \n",
    "Date: 19/07/2024\n",
    "\n",
    "This script can read in excel file(s) from sharepoint that have previously been extracted from 3dx, processed to add the additional functions/metrics and passed to smartsheets.  Currently, this is what it is doing as I am expecting to process the historic files to create summary metrics.\n",
    "\n",
    "Going forward this could read in the 3dx extract directly to validate the latest attributes.\n",
    "\n",
    "It can process as many files as you want, for as many product structures as you want, and build a history of data quality.  Currently writing out to excel file named the same as the input file and suffixed with '_validated'.\n",
    "\n",
    "Required Inputs:\n",
    "* 3dx extract xlsx file - as long as the standard columns used for calculating the metrics are present this will process it.\n",
    "\n",
    "Outputs Produced:\n",
    "* xlsx spreadsheet with full input BOM written to a sheet with columns for each metric appended to far right columns for reporting in power bi/excel\n",
    "* Additional sheets written into xlsx for each validation rule, showing the subset of records that failed each rule (might be useful for quick dashboard displays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Makes without Buys\n",
    "For each Make part there should be an assembly that needs at least one Buy part below it to make sense - if you're going to bake a cake, you need at least 1 ingredient!  If you're buying a cake, then you don't need anything else!\n",
    "\n",
    "If a MAKE (source code 'AIH','MIH','MOB') is not followed by a child BUY this is a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level'])].index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    df['make_no_buy'] = np.where((df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level']), True, False)\n",
    "\n",
    "    return df, make_no_buy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parent Source Code\n",
    "\n",
    "We will need to track the source code of the parent part and use it for validate checks coming later.\n",
    "\n",
    "This interates through the dataframe and appends to each row the parent part source code (the level numerically above this row's level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_source_code(df):\n",
    "    prev_level = 0\n",
    "\n",
    "    level_source = {}\n",
    "\n",
    "    for i, x in df.iterrows():\n",
    "        # take the current level source and store it\n",
    "        level_source[x['Level']] = x['Source Code']\n",
    "        if ((x['Level'] >= 4)):\n",
    "            df.loc[i, 'Parent Source Code'] = level_source[x['Level'] - 1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Source Code within parent checks\n",
    "\n",
    "sc_check_list is a list of invalid scenarios we are checking for with syntax: SOURCE CODE_PARENT SOURCE CODE\n",
    "\n",
    "A dataframe of invalid rows is written to dict_checks[sc_check] and a column is added to the end of the main dataframe with the sc_check as a column name holding true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_code_within_parent_checks(dict_checks, df):\n",
    "    # check for combinations of source codes within a parent source that's not accepted\n",
    "    sc_check_list = ['AIH_POA','BOP_FIP','FAS_FAS','FIP_FIP','FIP_FAS']\n",
    "    \n",
    "    for sc_check in sc_check_list:\n",
    "        sc, parent_sc = sc_check.split('_')\n",
    "\n",
    "        dict_checks[sc_check] = df[(df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc)]\n",
    "\n",
    "        df[sc_check] = np.where((df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc), True, False)\n",
    "\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Level 4 Source Code Checks\n",
    "\n",
    "level 4 (assembly level when first level = 0) should only have Source Code 'MIH' or 'AIH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_level_4_source_code_checks(dict_checks, df):\n",
    "    # level 4 can only be MIH or AIH\n",
    "    dict_checks['Non_MIH_AIH_Level_4'] = df[(df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH']))]\n",
    "    df['Non_MIH_AIH_Level_4'] = np.where((df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Fasteners with wrong parent source code\n",
    "\n",
    "Fasteners should only be within parents of 'FIP','AIH,'MIH'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAS_wrong_parent_source_code(dict_checks, df):\n",
    "    # FAS can only be within a FIP, AIH or MIH parent\n",
    "    dict_checks['FAS_Wrong_Parent_Source_code'] = df[(df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH']))]\n",
    "    df['FAS_Wrong_Parent_Source_code'] = np.where((df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Fastener checks\n",
    "\n",
    "Look for scenarios where a description says washer, bolt or grommet but the source code says 'BOF'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastener_checks(dict_checks, df):\n",
    "    # All BOF records that are fasteners should be {FAS}teners in the BOMS\n",
    "    # Part Description contains washer, bolt, grommet\n",
    "    # Source code = \"BOF\"\n",
    "    fastener_check_list = ['^washer|^bolt|^grommet']        \n",
    "\n",
    "    dict_checks['FAS_as_BOF'] = df[(df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF')]\n",
    "    df['FAS_as_BOF'] = np.where((df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Filter check columns\n",
    "\n",
    "For writing out to excel on separate sheets, only need to keep the pertinent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_check_columns(dict_checks):\n",
    "    # reduce the selection of columns used for writing out later\n",
    "    check_columns = [\n",
    "    'orig_sort',\n",
    "    'Last Modified By',\n",
    "    'Owner',\n",
    "    'Function Group',\n",
    "    'System',\n",
    "    'Sub System',\n",
    "    'Level',\n",
    "    'Title',\n",
    "    'Revision',\n",
    "    'Description',\n",
    "    'Parent Part',\n",
    "    'Source Code',\n",
    "    'Quantity',\n",
    "    'Parent Source Code'\n",
    "    ]\n",
    "\n",
    "    for key in dict_checks.keys():\n",
    "        print (key)\n",
    "        dict_checks[key] = dict_checks[key][check_columns]\n",
    "\n",
    "    return dict_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Validate GMT Standards 3DX Attributes Requirements\n",
    "\n",
    "Read in the GMT Standards document from GMT sharepoint folder and confirm 3dx extract contains the same columns and valid values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_attributes(df, attr_filename):\n",
    "\n",
    "\n",
    "    attr = pd.read_excel(attr_filename, sheet_name='Drop Down Attributes', na_values=\"\", keep_default_na=False)\n",
    "\n",
    "    # create a dictionary of all the valid values for each column - this drops the nan values for each column\n",
    "    attr_d = {attr[column].name: [y for y in attr[column] if not pd.isna(y)] for column in attr}\n",
    "\n",
    "    for key in attr_d:\n",
    "        # check the column exists\n",
    "        try:\n",
    "            mask = df[key].isin(attr_d[key])\n",
    "            df[key + ' Check'] = np.where(mask, 'Valid','Invalid')\n",
    "        except KeyError as e:\n",
    "            df[key + ' Check'] = 'Not in Extract'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.9 Validate BoM and Function Group Structure GMT document\n",
    "\n",
    "This document is stored on GMT-EngineeringBoM sharepoint in GMT Standards folder:  \n",
    "https://forsevengroup.sharepoint.com/:x:/r/sites/GMT-EngineeringBoM/Shared%20Documents/GMT%20-%20Standards/BoM%20and%20Function%20Group%20Structure%20GMT.xlsx?d=wc3cbfc77631c40b69ba7d5026066a2e7&csf=1&web=1&e=B64OP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_BOM_Function_Group_structure():\n",
    "    # this is incomplete\n",
    "    struct_filename = 'BoM and Function Group Structure GMT.xlsx'\n",
    "\n",
    "    struct = pd.read_excel(struct_filename, sheet_name='T48E')\n",
    "    # drop first row of struct which should be project, model variant, function group area, systems, sub sytems, AMs\n",
    "    struct = struct.loc[1:]\n",
    "\n",
    "    # find the first nan row in Level 4s as this will show where the last row in the valid values ends\n",
    "    last_row = struct['Level 4'].isna().idxmax()-1\n",
    "\n",
    "    # drop the rest of the struct rows\n",
    "    struct = struct.loc[:last_row]\n",
    "\n",
    "    # and now create a dictionary of all the valid values for each column - this drops the nan values for each column where we read merged cells from excel\n",
    "    struct_d = {struct[column].name: [y for y in struct[column] if not pd.isna(y)] for column in struct}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10 Validate Part No\n",
    "\n",
    "at the moment the Part Number (Title) should be:\n",
    "\n",
    "[project]-[functional area][5 digit part number] == 11 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_part_no(df):\n",
    "    # ? means the preceding bracketed group is optional (optional s,S and trailing X)\n",
    "    pattern = r'([A-Z]\\d{2}[e])-(\\w[A-Za-z0-9]*)?-?([A-Z])(\\d{5})(X)?'\n",
    "    df[['extr_project','extr_invalid_code','extr_function','extr_pn','extr_maturity']] = df['Title'].str.extract(pattern, expand=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Script config setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import xlwings as xw\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import platform\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to determine whether we're running in Juypter notebook or as a command line script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_metrics(df):\n",
    "    for col in ['UOM','Provide','Source Code','CAD Mass','CAD Maturity','CAD Material','Electrical Connector']:\n",
    "        df['Missing {}'.format(col)] = np.where(df[col].isnull(), 1, 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAD_Material_validation(df):\n",
    "    df[1:][df['Title'].str.contains('TPP', na=False)].groupby(['Title','CAD Material']).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bi_key(df):\n",
    "    # for use in power_bi reporting\n",
    "    # replace NaN with ''\n",
    "    df['bi_combined_key'] = df['Product'].astype(str) + df['Function Group'].astype(str) + df['System'].astype(str) + df['Sub System'].astype(str)\n",
    "    \n",
    "    return df['bi_combined_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the folder structure based on whether we're running on a test windows pc, in azure server, a mac, or in the real world against sharepoint - helps Mark test on different devices! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_folder_defaults():\n",
    "    if 'macOS' in platform.platform():\n",
    "        # set some defaults for testing on mac\n",
    "        download_dir = Path('/Users/mark/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir\n",
    "\n",
    "    elif 'Server' in platform.platform():\n",
    "        # we're on the azure server (probably)\n",
    "        user_dir = Path('Z:/python/FilesIn')\n",
    "\n",
    "        download_dir = Path(user_dir)\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = Path('Z:/python/FilesOut')\n",
    "\n",
    "    elif os.getlogin() == 'mark_':\n",
    "        # my test windows machine\n",
    "        download_dir = Path('C:/Users/mark_/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir        \n",
    "\n",
    "    else:\n",
    "        # personal one drive\n",
    "        user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "        # replace USERNAME with current logged on user\n",
    "        user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "        # read in config file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('user_directory.ini')\n",
    "\n",
    "        # read in gm_dir and gm_docs from config file\n",
    "        gm_dir = Path(config[os.getlogin().lower()]['gm_dir'])\n",
    "        gm_docs = Path(config[os.getlogin().lower()]['gmt'])\n",
    "        # this may find more than one sharepoint directory\n",
    "        # sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "        sharepoint_dir = Path(user_dir / gm_dir / gm_docs)\n",
    "\n",
    "        # download_dir = os.path.join(sharepoint_dir, 'Data Shuttle', 'downloads')\n",
    "        download_dir = Path(sharepoint_dir / 'Data Shuttle' / 'downloads')\n",
    "\n",
    "    return sharepoint_dir, download_dir, user_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the folder defaults look for the files we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(download_dir):\n",
    "    # find any changed files changed in past 2hrs in the downloads directory\n",
    "    dirpath = download_dir\n",
    "    files = []\n",
    "    for p, ds, fs in os.walk(dirpath):\n",
    "        for fn in fs:\n",
    "            if 'Updated_' in fn:\n",
    "                # was using this to filter what filenames to find\n",
    "                filepath = os.path.join(p, fn)\n",
    "                files.append(filepath)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_variant(search):\n",
    "\n",
    "    variant_d = {'T48E-01-Z00001':'VP_5_door',\n",
    "                'T48E-01-Z00005':'XP_5_door',\n",
    "                'T48E-02-Z00001':'VP_3_door',\n",
    "                'T48E-02-Z00005':'XP_3_door'}\n",
    "    try:\n",
    "        variant = variant_d[search]\n",
    "    except KeyError:    \n",
    "        print (\"No variant lookup found for {} Therefore didn't update variant name\".format(search))\n",
    "        # just return what we searched with\n",
    "        variant = search\n",
    "    return variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Write to excel\n",
    "\n",
    "Call xlwings with your pre-prepared dictionary and write out many sheets to one excel file, naming the sheets whatever you called your dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl(output_file, dict_checks):\n",
    "\n",
    "    outfile = output_file\n",
    "    df_dict = dict_checks\n",
    "\n",
    "    import xlwings as xw\n",
    "    try:\n",
    "        wb = xw.Book(output_file)\n",
    "        print (\"writing to existing {}\".format(outfile))\n",
    "    except FileNotFoundError:\n",
    "        # create a new book\n",
    "        print (\"creating new {}\".format(outfile))\n",
    "        wb = xw.Book()\n",
    "        wb.save(outfile)\n",
    "\n",
    "    for key in df_dict.keys():\n",
    "        try:\n",
    "            ws = wb.sheets.add(key)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "        \n",
    "        ws = wb.sheets[key]\n",
    "\n",
    "        table_name = key\n",
    "\n",
    "        ws.clear()\n",
    "\n",
    "        df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "        if len(df) > 0:\n",
    "            if table_name in [table.df for table in ws.tables]:\n",
    "                ws.tables[table_name].update(df)\n",
    "            else:\n",
    "                table_name = ws.tables.add(source=ws['A1'],\n",
    "                                            name=table_name).update(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out to excel using sub system\n",
    "\n",
    "This was used previously (GMD) might be useful again so haven't removed, but not currently calling.\n",
    "\n",
    "Writes out the checks to sheets filtered against the sub system - maybe useful if we wanted to give the problem rows to a team to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl_sub_system(dict_checks):\n",
    "    sub_sys = dict_checks[check]['Sub System'].unique()\n",
    "    sub_sys.sort()\n",
    "\n",
    "    for s_sys in sub_sys:\n",
    "\n",
    "        df_temp = dict_checks[check][dict_checks[check]['Sub System'] == s_sys]\n",
    "\n",
    "        if df_temp.shape[0] > 0:\n",
    "            df_temp.to_excel(writer, sheet_name=s_sys, index=False)\n",
    "\n",
    "            ws = writer.sheets[s_sys]\n",
    "            wb = writer.book\n",
    "\n",
    "            excel_formatting.adjust_col_width_from_col(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main Processing\n",
    "\n",
    "This is where the processing begins, and where we call the functions defined above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df):\n",
    "\n",
    "    # copy df without the 1st row of metrics and 1st col of BOM COUNTs\n",
    "    df = df.iloc[1:,1:]        \n",
    "\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.rename(columns={'index':'bom_order'}, inplace=True)\n",
    "\n",
    "    # variant ie T48E-01-Z00001\n",
    "    # variant should be the first title in this dataframe, or the level 0 title\n",
    "    variant = lookup_variant(df['Title'][df['Level']==0].values[0])\n",
    "    # product ie T48E should be first part of the Title we got in variant\n",
    "    product = re.split('-', df['Title'][df['Level']==0].values[0])[0].upper()\n",
    "\n",
    "\n",
    "    # add variant column for merging multiple BOMs together and reporting on 1 dashboard\n",
    "    df['Variant'] = variant\n",
    "    df['Product'] = product\n",
    "    \n",
    "    df['bi_combined_key'] = add_bi_key(df)    \n",
    "\n",
    "    df = add_missing_metrics(df)\n",
    "    # add part validation\n",
    "    df = validate_part_no(df)\n",
    "\n",
    "    # add parent source code to each row for validation checks to come\n",
    "    df = parent_source_code(df)\n",
    "\n",
    "    # initialise a dictionary to store all the check output as dataframes\n",
    "    dict_checks = {}\n",
    "\n",
    "    # complete the source code with parent source code checks\n",
    "    dict_checks, df = source_code_within_parent_checks(dict_checks, df)\n",
    "\n",
    "    # check all level 4 have the correct source code\n",
    "    dict_checks, df = check_level_4_source_code_checks(dict_checks, df)\n",
    "\n",
    "    # check for FAS with the wrong source code\n",
    "    dict_checks, df = FAS_wrong_parent_source_code(dict_checks, df)\n",
    "\n",
    "    # complete the fasteners source code checks\n",
    "    dict_checks, df = fastener_checks(dict_checks, df)\n",
    "\n",
    "    # look for make assemblys with no parts to buy\n",
    "    df, make_no_buy = check_make_no_buy(df)\n",
    "    dict_checks['make_no_buy'] = df.loc[make_no_buy]\n",
    "\n",
    "    # validate the 3dx attributes that have dropdowns\n",
    "    attr_file = Path(sharepoint_dir / 'GMT - Standards' / '3DX Attributes Requirements for Release and Clarification.xlsx')\n",
    "    df = check_attributes(df, attr_file)\n",
    "\n",
    "    # write out just the cols we need to report against\n",
    "    dict_checks = filter_check_columns(dict_checks)\n",
    "\n",
    "    # add the full df to the sheet\n",
    "    dict_checks['BOM'] = df\n",
    "\n",
    "    # build outfile name and write to excel for power bi\n",
    "    outfile_name = product + '_' + variant\n",
    "    output_file = Path(sharepoint_dir) / Path(outfile_name + '_power_bi_metrics').with_suffix('.xlsx')\n",
    "    write_to_xl(output_file, dict_checks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIH_POA\n",
      "BOP_FIP\n",
      "FAS_FAS\n",
      "FIP_FIP\n",
      "FIP_FAS\n",
      "Non_MIH_AIH_Level_4\n",
      "FAS_Wrong_Parent_Source_code\n",
      "FAS_as_BOF\n",
      "make_no_buy\n",
      "writing to existing C:\\Users\\mark_\\Downloads\\T48E_XP_5_door_power_bi_metrics.xlsx\n",
      "Sheet named 'AIH_POA' already present in workbook\n",
      "Sheet named 'BOP_FIP' already present in workbook\n",
      "Sheet named 'FAS_FAS' already present in workbook\n",
      "Sheet named 'FIP_FIP' already present in workbook\n",
      "Sheet named 'FIP_FAS' already present in workbook\n",
      "Sheet named 'Non_MIH_AIH_Level_4' already present in workbook\n",
      "Sheet named 'FAS_Wrong_Parent_Source_code' already present in workbook\n",
      "Sheet named 'FAS_as_BOF' already present in workbook\n",
      "Sheet named 'make_no_buy' already present in workbook\n",
      "Sheet named 'BOM' already present in workbook\n"
     ]
    }
   ],
   "source": [
    "# this gets called if running from this script.  \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # for reading in multiple files\n",
    "\n",
    "    # files = find_files()\n",
    "    dict_df = {}\n",
    "\n",
    "    filename = 'Updated_T48e-01-Z00005_2024-07-26.xlsx'\n",
    "\n",
    "    sharepoint_dir, download_dir, user_dir = set_folder_defaults()\n",
    "\n",
    "    file = Path(download_dir) / filename\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        # reading in the historic excel files\n",
    "        df = pd.read_excel(f, parse_dates=True)\n",
    "        f.close()\n",
    "\n",
    "    # call the main processing\n",
    "    main(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Development\n",
    "\n",
    "Dumping ground for checks that might come in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Level 4 ASSY\n",
    "\n",
    "Should any part with ASSY in description be at Level 4 only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Non_level_4_ASSY(df):\n",
    "    df[df.Description.str.contains('ASSY', na=False)].groupby(['Level']).size()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Source Codes\n",
    "\n",
    "Check whether a part has been configured with more than one source code within the product structure\n",
    "\n",
    "Is it valid to have a TFF part that's FAS and POA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_source_code(df):\n",
    "    unstacked = df.groupby(['Title','Revision','Source Code']).size().unstack()\n",
    "\n",
    "    # find number of columns dynamically, as number of unique status controls the number of columns\n",
    "    expected_status_count = len(unstacked.columns) - 1\n",
    "    unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_status_count]\n",
    "    unstacked2\n",
    "\n",
    "\n",
    "    multi_sc = unstacked2.reset_index().fillna('')\n",
    "\n",
    "    # make_sc_cols = ['AIH','MIH','MOB']\n",
    "\n",
    "    first_cols = ['Title', 'Revision']\n",
    "\n",
    "    cols_to_order = first_cols\n",
    "    sc_ordered_cols = cols_to_order + (multi_sc.columns.drop(cols_to_order).tolist())\n",
    "\n",
    "    multi_sc = multi_sc[sc_ordered_cols]\n",
    "\n",
    "    return multi_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(df, figsize):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "       \n",
    "    hmap = plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(df, annot = True, fmt=\".0%\", cmap='YlGnBu', annot_kws={'fontsize':8}, linewidths=0.5)\n",
    "    ax.set(xlabel=\"\", ylabel=\"\")\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.rc('xtick', labelsize=10)\n",
    "    plt.rc('ytick', labelsize=10)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_ticks([0, .2, .75, 1])\n",
    "    cbar.set_ticklabels(['0%', '20%', '75%', '100%'])\n",
    "    plt.figure()\n",
    "    # sns.set(font_scale=.5)\n",
    "    # plt.show()\n",
    "    plt.close(hmap)\n",
    "    return hmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
