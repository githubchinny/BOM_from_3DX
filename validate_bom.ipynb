{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate BoM\n",
    "\n",
    "This script reads in an excel file(s) from sharepoint that have previously been extracted from 3dx, processed to add the additional functions/metrics and passed to smartsheets.\n",
    "\n",
    "It can process as many files as you want for as many product structures as you want and build a history of data quality.  Currently writing out to a file named the same as the input file + '_validated' suffixed to the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import xlwings as xw\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import platform\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to determine whether we're running in Juypter notebook or as a command line script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the folder structure based on whether we're running on a test windows pc, in azure server, a mac, or in the real world against sharepoint - helps Mark test on different devices! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_folder_defaults():\n",
    "    if 'macOS' in platform.platform():\n",
    "        # set some defaults for testing on mac\n",
    "        download_dir = Path('/Users/mark/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir\n",
    "\n",
    "    elif 'Server' in platform.platform():\n",
    "        # we're on the azure server (probably)\n",
    "        user_dir = Path('Z:/python/FilesIn')\n",
    "\n",
    "        download_dir = Path(user_dir)\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = Path('Z:/python/FilesOut')\n",
    "\n",
    "    elif os.getlogin() == 'mark_':\n",
    "        # my test windows machine\n",
    "        download_dir = Path('C:/Users/mark_/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir        \n",
    "\n",
    "    else:\n",
    "        # personal one drive\n",
    "        user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "        # replace USERNAME with current logged on user\n",
    "        user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "        # read in config file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('user_directory.ini')\n",
    "\n",
    "        # read in gm_dir and gm_docs from config file\n",
    "        gm_dir = Path(config[os.getlogin().lower()]['gm_dir'])\n",
    "        gm_docs = Path(config[os.getlogin().lower()]['gmt'])\n",
    "        # this may find more than one sharepoint directory\n",
    "        # sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "        sharepoint_dir = Path(user_dir / gm_dir / gm_docs)\n",
    "\n",
    "        # download_dir = os.path.join(sharepoint_dir, 'Data Shuttle', 'downloads')\n",
    "        download_dir = Path(sharepoint_dir / 'Data Shuttle' / 'downloads')\n",
    "\n",
    "    return sharepoint_dir, download_dir, user_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the folder defaults look for the files we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(download_dir):\n",
    "    # find any changed files changed in past 2hrs in the downloads directory\n",
    "    dirpath = download_dir\n",
    "    files = []\n",
    "    for p, ds, fs in os.walk(dirpath):\n",
    "        for fn in fs:\n",
    "            if 'Updated_' in fn:\n",
    "                # was using this to filter what filenames to find\n",
    "                filepath = os.path.join(p, fn)\n",
    "                files.append(filepath)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makes without Buys\n",
    "For each Make part there should be an assembly that needs at least one Buy part below it to make sense - if you're going to bake a cake, you need at least 1 ingredient!  If you're buying a cake, then you don't need anything else!\n",
    "\n",
    "If a MAKE is not followed by a child BUY this is a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level'])].index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    df['make_no_buy'] = np.where((df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level']), True, False)\n",
    "\n",
    "    return df, make_no_buy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parent Source Code\n",
    "\n",
    "We will need to track what the source code of the parent part is and validate whether it is a correct scenario.\n",
    "\n",
    "This interates through the dataframe and writes out to each row what the parent part (the level above this row's level) source code was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_source_code(df):\n",
    "    prev_level = 0\n",
    "\n",
    "    level_source = {}\n",
    "\n",
    "    for i, x in df.iterrows():\n",
    "        # take the current level source and store it\n",
    "        level_source[x['Level']] = x['Source Code']\n",
    "        if ((x['Level'] >= 4)):\n",
    "            df.loc[i, 'Parent Source Code'] = level_source[x['Level'] - 1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code within parent checks\n",
    "\n",
    "sc_check_list is a list of invalid scenarios we are checking for with syntax: SOURCE CODE_PARENT SOURCE CODE\n",
    "\n",
    "A dataframe of invalid rows is written to dict_checks[sc_check] and a column is added to the end of the main dataframe with the sc_check as a column name holding true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_code_within_parent_checks(dict_checks, df):\n",
    "    # check for combinations of source codes within a parent source that's not accepted\n",
    "    sc_check_list = ['AIH_POA','BOP_FIP','FAS_FAS','FIP_FIP','FIP_FAS']\n",
    "    \n",
    "    for sc_check in sc_check_list:\n",
    "        sc, parent_sc = sc_check.split('_')\n",
    "\n",
    "        dict_checks[sc_check] = df[(df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc)]\n",
    "\n",
    "        df[sc_check] = np.where((df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc), True, False)\n",
    "\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 4 Source Code Checks\n",
    "\n",
    "level 4 (assembly level when first level = 0) should only have Source Code 'MIH' or 'AIH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_level_4_source_code_checks(dict_checks, df):\n",
    "    # level 4 can only be MIH or AIH\n",
    "    dict_checks['Non_MIH_AIH_Level_4'] = df[(df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH']))]\n",
    "    df['Non_MIH_AIH_Level_4'] = np.where((df['Level'] == 4) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasteners with wrong parent source code\n",
    "\n",
    "Fasteners should only be within parents of 'FIP','AIH,'MIH'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAS_wrong_parent_source_code(dict_checks, df):\n",
    "    # FAS can only be within a FIP, AIH or MIH parent\n",
    "    dict_checks['FAS_Wrong_Parent_Source_code'] = df[(df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH']))]\n",
    "    df['FAS_Wrong_Parent_Source_code'] = np.where((df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastener checks\n",
    "\n",
    "Look for scenarios where a description says washer, bolt or grommet but the source code says 'BOF'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastener_checks(dict_checks, df):\n",
    "    # All BOF records that are fasteners should be {FAS}teners in the BOMS\n",
    "    # Part Description contains washer, bolt, grommet\n",
    "    # Source code = \"BOF\"\n",
    "    fastener_check_list = ['^washer|^bolt|^grommet']        \n",
    "\n",
    "    dict_checks['FAS_as_BOF'] = df[(df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF')]\n",
    "    df['FAS_as_BOF'] = np.where((df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter check columns\n",
    "\n",
    "For writing out to excel on separate sheets, only need to keep the pertinent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_check_columns(dict_checks):\n",
    "    # reduce the selection of columns used for writing out later\n",
    "    check_columns = [\n",
    "    'orig_sort',\n",
    "    'Function Group',\n",
    "    'System',\n",
    "    'Sub System',\n",
    "    'Level',\n",
    "    'Title',\n",
    "    'Revision',\n",
    "    'Description',\n",
    "    'Parent Part',\n",
    "    'Source Code',\n",
    "    'Quantity',\n",
    "    'Parent Source Code'\n",
    "    ]\n",
    "\n",
    "    for key in dict_checks.keys():\n",
    "        print (key)\n",
    "        dict_checks[key] = dict_checks[key][check_columns]\n",
    "\n",
    "    return dict_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to excel\n",
    "\n",
    "Call xlwings with your pre-prepared dictionary and write out many sheets to one excel file, naming the sheets whatever you called your dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl(outfile, df_dict):\n",
    "    import xlwings as xw\n",
    "    with xw.App(visible=True) as app:\n",
    "        try:\n",
    "            wb = xw.Book(outfile)\n",
    "            print (\"writing to existing {}\".format(outfile))\n",
    "        except FileNotFoundError:\n",
    "            # create a new book\n",
    "            print (\"creating new {}\".format(outfile))\n",
    "            wb = xw.Book()\n",
    "            wb.save(outfile)\n",
    "\n",
    "        for key in df_dict.keys():\n",
    "            try:\n",
    "                ws = wb.sheets.add(key)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "            \n",
    "            ws = wb.sheets[key]\n",
    "\n",
    "            table_name = key\n",
    "\n",
    "            ws.clear()\n",
    "\n",
    "            df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "            if table_name in [table.df for table in ws.tables]:\n",
    "                ws.tables[table_name].update(df)\n",
    "            else:\n",
    "                table_name = ws.tables.add(source=ws['A1'],\n",
    "                                            name=table_name).update(df)\n",
    "    wb.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out to excel using sub system\n",
    "\n",
    "This was used previously (GMD) might be useful again so haven't removed, but not currently calling.\n",
    "\n",
    "Writes out the checks to sheets filtered against the sub system - maybe useful if we wanted to give the problem rows to a team to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl_sub_system(dict_checks):\n",
    "    sub_sys = dict_checks[check]['Sub System'].unique()\n",
    "    sub_sys.sort()\n",
    "\n",
    "    for s_sys in sub_sys:\n",
    "\n",
    "        df_temp = dict_checks[check][dict_checks[check]['Sub System'] == s_sys]\n",
    "\n",
    "        if df_temp.shape[0] > 0:\n",
    "            df_temp.to_excel(writer, sheet_name=s_sys, index=False)\n",
    "\n",
    "            ws = writer.sheets[s_sys]\n",
    "            wb = writer.book\n",
    "\n",
    "            excel_formatting.adjust_col_width_from_col(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Processing\n",
    "\n",
    "This is where the processing begins, and where and in which order we call the functions defined above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIH_POA\n",
      "BOP_FIP\n",
      "FAS_FAS\n",
      "FIP_FIP\n",
      "FIP_FAS\n",
      "Non_MIH_AIH_Level_4\n",
      "FAS_Wrong_Parent_Source_code\n",
      "FAS_as_BOF\n",
      "make_no_buy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # for reading in multiple files\n",
    "\n",
    "    # files = find_files()\n",
    "    dict_df = {}\n",
    "\n",
    "    filename = 'Updated_T48e-01-Z00005_2024-07-19.xlsx'\n",
    "    sharepoint_dir, download_dir, user_dir = set_folder_defaults()\n",
    "\n",
    "    file = Path(download_dir) / filename\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        # reading in the historic excel files\n",
    "        df = pd.read_excel(f, parse_dates=True)\n",
    "        f.close()\n",
    "\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.rename(columns={'index':'bom_order'}, inplace=True)\n",
    "\n",
    "    # add parent source code to each row for validation checks to come\n",
    "    df = parent_source_code(df)\n",
    "\n",
    "    # initialise a dictionary to store all the check output as dataframes\n",
    "    dict_checks = {}\n",
    "\n",
    "    # complete the source code with parent source code checks\n",
    "    dict_checks, df = source_code_within_parent_checks(dict_checks, df)\n",
    "\n",
    "    # check all level 4 have the correct source code\n",
    "    dict_checks, df = check_level_4_source_code_checks(dict_checks, df)\n",
    "\n",
    "    # check for FAS with the wrong source code\n",
    "    dict_checks, df = FAS_wrong_parent_source_code(dict_checks, df)\n",
    "\n",
    "    # complete the fasteners source code checks\n",
    "    dict_checks, df = fastener_checks(dict_checks, df)\n",
    "\n",
    "    # look for make assemblys with no parts to buy\n",
    "    df, make_no_buy = check_make_no_buy(df)\n",
    "    dict_checks['make_no_buy'] = df.loc[make_no_buy]\n",
    "\n",
    "    # write out just the cols we need to report against\n",
    "    dict_checks = filter_check_columns(dict_checks)\n",
    "\n",
    "    # add the full df to the sheet\n",
    "    dict_checks['BOM'] = df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release status checks\n",
    "For part levels >=4 groupby part number and release status and report which parts have more than 1 release status - should be unique per part number\n",
    "\n",
    "| \t|\t|\t|Function Group|\tPart Level|\tIssue Level|\trow|\n",
    "|---|---|---|--------------|--------------|------------|-------|\n",
    "|Part Number|\tSub Group|\tRelease Status|||||\t\t\t\t\n",
    "|T33-A1117X\t|A01-Structure Systems\t|REL|\tT33-BoM-XP\t|7.0\t|1.0\t|617|\n",
    "|||AWT\t|T33-BoM-XP\t|6.0\t|1.0\t|1284|\n",
    "|T33-A1475\t|A02-Panels & Closure Systems\t|AWT\t|T33-BoM-XP\t|6.0\t|1.0\t|137|\n",
    "|||REL\t|T33-BoM-XP\t|6.0\t|1.0\t|230|\n",
    "|T33-A1476X\t|A02-Panels & Closure Systems\t|AWT\t|T33-BoM-XP\t|6.0\t|1.0\t|143|\n",
    "\n",
    "\n",
    "This is written out to excel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the validation checks to excel\n",
    "\n",
    "- collated bom multi status   \n",
    "- wrong parent parts   \n",
    "- collated and cleaned bom written out with cleaned part numbers, parent parts and release status attached   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can add a list of source code combinations to check as we find out they are not valid (mainly causing a problem for IFS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out source code checks to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to existing c:\\Users\\mark_\\Documents\\GitHub\\BOM_from_3DX\\Updated_T48e-01-Z00005_2024-07-19_validated.xlsx\n",
      "Sheet named 'AIH_POA' already present in workbook\n",
      "Sheet named 'BOP_FIP' already present in workbook\n",
      "Sheet named 'FAS_FAS' already present in workbook\n",
      "Sheet named 'FIP_FIP' already present in workbook\n",
      "Sheet named 'FIP_FAS' already present in workbook\n",
      "Sheet named 'Non_MIH_AIH_Level_4' already present in workbook\n",
      "Sheet named 'FAS_Wrong_Parent_Source_code' already present in workbook\n",
      "Sheet named 'FAS_as_BOF' already present in workbook\n",
      "Sheet named 'make_no_buy' already present in workbook\n",
      "Sheet named 'BOM' already present in workbook\n"
     ]
    }
   ],
   "source": [
    "# Write out to excel\n",
    "pathfile = Path(file.name).stem\n",
    "output_file = Path(sys.path[0]) / Path(pathfile + '_validated').with_suffix('.xlsx')\n",
    "write_to_xl(output_file, dict_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIH_POA\n",
      "363      933\n",
      "388      965\n",
      "647     1311\n",
      "2747    4957\n",
      "2749    4959\n",
      "Name: orig_sort, dtype: object\n",
      "BOP_FIP\n",
      "Series([], Name: orig_sort, dtype: object)\n",
      "FAS_FAS\n",
      "297     496\n",
      "316     541\n",
      "405     995\n",
      "427    1023\n",
      "428    1029\n",
      "Name: orig_sort, dtype: object\n",
      "FIP_FIP\n",
      "Series([], Name: orig_sort, dtype: object)\n",
      "FIP_FAS\n",
      "Series([], Name: orig_sort, dtype: object)\n",
      "FAS_as_BOF\n",
      "11    10\n",
      "27    39\n",
      "31    52\n",
      "37    66\n",
      "38    67\n",
      "Name: orig_sort, dtype: object\n",
      "make_no_buy\n",
      "23      35\n",
      "78     159\n",
      "95     179\n",
      "100    184\n",
      "102    186\n",
      "Name: orig_sort, dtype: object\n",
      "BOM\n",
      "0    percent_missing\n",
      "1                  0\n",
      "2                  1\n",
      "3                  2\n",
      "4                  3\n",
      "Name: orig_sort, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for key in dict_checks.keys():\n",
    "    print (key)\n",
    "    print (dict_checks[key].orig_sort.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxlwings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxw\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_column_letter\n\u001b[1;32m----> 4\u001b[0m make_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sharepoint_dir, project, project \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_MakeBuyQuery.xlsm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m xw\u001b[38;5;241m.\u001b[39mApp():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project' is not defined"
     ]
    }
   ],
   "source": [
    "import xlwings as xw\n",
    "from openpyxl.utils.cell import get_column_letter\n",
    "\n",
    "make_filename = os.path.join(sharepoint_dir, project, project + '_MakeBuyQuery.xlsm')\n",
    "\n",
    "with xw.App():\n",
    "    try:\n",
    "        wb = xw.Book(make_filename)\n",
    "    except:\n",
    "        wb = xw.Book()\n",
    "    \n",
    "    ws = wb.sheets[0]\n",
    "\n",
    "    startrow=0\n",
    "\n",
    "    ws.range(\"A:XX\").clear()\n",
    "\n",
    "    ws['A1'].options(pd.DataFrame, header=1, index=False).value=existing_bom\n",
    "\n",
    "    last_col_letter = get_column_letter(existing_bom.shape[1])\n",
    "\n",
    "\n",
    "    for row in make_no_buy:\n",
    "        xw.Range('A{}:{}{}'.format(row + 2, last_col_letter, row + 2)).color = (69, 165, 237)\n",
    "\n",
    "    ws.tables.add(ws.used_range, name=\"a_table\")\n",
    "\n",
    "    # autofit the columns\n",
    "    ws.autofit('c')\n",
    "    wb.save(make_filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Source Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_source_code(df):\n",
    "\n",
    "    unstacked = df.groupby(['Part Number','Issue Level','Function Group','Sub Group','Source Code']).size().unstack()\n",
    "\n",
    "    # find number of columns dynamically, as number of unique status controls the number of columns\n",
    "    expected_status_count = len(unstacked.columns) - 1\n",
    "    unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_status_count]\n",
    "    unstacked2\n",
    "\n",
    "\n",
    "    multi_sc = unstacked2.reset_index().fillna('')\n",
    "\n",
    "    make_sc_cols = ['AIH','MIH','MOB']\n",
    "\n",
    "    first_cols = ['Part Number', 'Issue Level', 'Function Group', 'Sub Group']\n",
    "\n",
    "    cols_to_order = first_cols + make_sc_cols\n",
    "    sc_ordered_cols = cols_to_order + (multi_sc.columns.drop(cols_to_order).tolist())\n",
    "\n",
    "    multi_sc = multi_sc[sc_ordered_cols]\n",
    "\n",
    "    return multi_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sc = multi_source_code(existing_bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "\n",
    "multi_sc_filename = os.path.join(base, '{}-Multi-Source-Codes.xlsm'.format(project))\n",
    "\n",
    "# wb = xw.Book(path)\n",
    "\n",
    "# open file if it exists\n",
    "try:\n",
    "    wb = xw.Book(multi_sc_filename)\n",
    "# otherwise create a new file    \n",
    "except:\n",
    "    wb = xw.Book()\n",
    "\n",
    "ws = wb.sheets[0]\n",
    "\n",
    "ws.clear_contents()\n",
    "\n",
    "# autofit the columns\n",
    "wb.sheets[ws].autofit('c')\n",
    "\n",
    "startrow=0\n",
    "\n",
    "ws['A1'].options(pd.DataFrame, header=1, index=False).value=multi_sc\n",
    "\n",
    "# wb.save(multi_sc_filename) - this errors - not sure it's needed with autosave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
