{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate \n",
    "Author: Mark Chinnock  \n",
    "Date: 19/07/2024\n",
    "\n",
    "This script reads in an excel file(s) from sharepoint that have previously been extracted from 3dx, processed to add the additional functions/metrics and passed to smartsheets.\n",
    "\n",
    "It can process as many files as you want, for as many product structures as you want, and build a history of data quality.  Currently writing out to excel file named the same as the input file and suffixed with '_validated'.\n",
    "\n",
    "Required Inputs:\n",
    "* 3dx extract xlsx file - as long as the standard columns used for calculating the metrics are present this will process it.\n",
    "\n",
    "Outputs Produced:\n",
    "* xlsx spreadsheet with full input BOM written to a sheet with columns for each metric appended to far right columns for reporting in power bi/excel\n",
    "* Additional sheets written into xlsx for each validation rule, showing the subset of records that failed each rule (might be useful for quick dashboard displays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Makes without Buys\n",
    "For each Make part there should be an assembly that needs at least one Buy part below it to make sense - if you're going to bake a cake, you need at least 1 ingredient!  If you're buying a cake, then you don't need anything else!\n",
    "\n",
    "If a MAKE (source code 'AIH','MIH','MOB') is not followed by a child BUY this is a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level'])].index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    df['make_no_buy'] = np.where((df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level']), True, False)\n",
    "\n",
    "    return df, make_no_buy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parent Source Code\n",
    "\n",
    "We will need to track the source code of the parent part and use it for validate checks coming later.\n",
    "\n",
    "This interates through the dataframe and appends to each row the parent part source code (the level numerically above this row's level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_source_code(df):\n",
    "    prev_level = 0\n",
    "\n",
    "    level_source = {}\n",
    "\n",
    "    for i, x in df.iterrows():\n",
    "        # take the current level source and store it\n",
    "        level_source[x['Level']] = x['Source Code']\n",
    "        if ((x['Level'] >= 4)):\n",
    "            df.loc[i, 'Parent Source Code'] = level_source[x['Level'] - 1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Source Code within parent checks\n",
    "\n",
    "sc_check_list is a list of invalid scenarios we are checking for with syntax: SOURCE CODE_PARENT SOURCE CODE\n",
    "\n",
    "A dataframe of invalid rows is written to dict_checks[sc_check] and a column is added to the end of the main dataframe with the sc_check as a column name holding true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_code_within_parent_checks(dict_checks, df):\n",
    "    # check for combinations of source codes within a parent source that's not accepted\n",
    "    sc_check_list = ['AIH_POA','BOP_FIP','FAS_FAS','FIP_FIP','FIP_FAS']\n",
    "    \n",
    "    for sc_check in sc_check_list:\n",
    "        sc, parent_sc = sc_check.split('_')\n",
    "\n",
    "        dict_checks[sc_check] = df[(df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc)]\n",
    "\n",
    "        df[sc_check] = np.where((df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc), True, False)\n",
    "\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Level 4 Source Code Checks\n",
    "\n",
    "level 4 (assembly level when first level = 0) should only have Source Code 'MIH' or 'AIH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_level_4_source_code_checks(dict_checks, df):\n",
    "    # level 4 can only be MIH or AIH\n",
    "    dict_checks['Non_MIH_AIH_Level_4'] = df[(df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH']))]\n",
    "    df['Non_MIH_AIH_Level_4'] = np.where((df['Level'] == 4) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Fasteners with wrong parent source code\n",
    "\n",
    "Fasteners should only be within parents of 'FIP','AIH,'MIH'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAS_wrong_parent_source_code(dict_checks, df):\n",
    "    # FAS can only be within a FIP, AIH or MIH parent\n",
    "    dict_checks['FAS_Wrong_Parent_Source_code'] = df[(df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH']))]\n",
    "    df['FAS_Wrong_Parent_Source_code'] = np.where((df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Fastener checks\n",
    "\n",
    "Look for scenarios where a description says washer, bolt or grommet but the source code says 'BOF'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastener_checks(dict_checks, df):\n",
    "    # All BOF records that are fasteners should be {FAS}teners in the BOMS\n",
    "    # Part Description contains washer, bolt, grommet\n",
    "    # Source code = \"BOF\"\n",
    "    fastener_check_list = ['^washer|^bolt|^grommet']        \n",
    "\n",
    "    dict_checks['FAS_as_BOF'] = df[(df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF')]\n",
    "    df['FAS_as_BOF'] = np.where((df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Filter check columns\n",
    "\n",
    "For writing out to excel on separate sheets, only need to keep the pertinent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_check_columns(dict_checks):\n",
    "    # reduce the selection of columns used for writing out later\n",
    "    check_columns = [\n",
    "    'orig_sort',\n",
    "    'Last Modified By',\n",
    "    'Owner',\n",
    "    'Function Group',\n",
    "    'System',\n",
    "    'Sub System',\n",
    "    'Level',\n",
    "    'Title',\n",
    "    'Revision',\n",
    "    'Description',\n",
    "    'Parent Part',\n",
    "    'Source Code',\n",
    "    'Quantity',\n",
    "    'Parent Source Code'\n",
    "    ]\n",
    "\n",
    "    for key in dict_checks.keys():\n",
    "        print (key)\n",
    "        dict_checks[key] = dict_checks[key][check_columns]\n",
    "\n",
    "    return dict_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Script config setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import xlwings as xw\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import platform\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to determine whether we're running in Juypter notebook or as a command line script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the folder structure based on whether we're running on a test windows pc, in azure server, a mac, or in the real world against sharepoint - helps Mark test on different devices! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_folder_defaults():\n",
    "    if 'macOS' in platform.platform():\n",
    "        # set some defaults for testing on mac\n",
    "        download_dir = Path('/Users/mark/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir\n",
    "\n",
    "    elif 'Server' in platform.platform():\n",
    "        # we're on the azure server (probably)\n",
    "        user_dir = Path('Z:/python/FilesIn')\n",
    "\n",
    "        download_dir = Path(user_dir)\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = Path('Z:/python/FilesOut')\n",
    "\n",
    "    elif os.getlogin() == 'mark_':\n",
    "        # my test windows machine\n",
    "        download_dir = Path('C:/Users/mark_/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir        \n",
    "\n",
    "    else:\n",
    "        # personal one drive\n",
    "        user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "        # replace USERNAME with current logged on user\n",
    "        user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "        # read in config file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('user_directory.ini')\n",
    "\n",
    "        # read in gm_dir and gm_docs from config file\n",
    "        gm_dir = Path(config[os.getlogin().lower()]['gm_dir'])\n",
    "        gm_docs = Path(config[os.getlogin().lower()]['gmt'])\n",
    "        # this may find more than one sharepoint directory\n",
    "        # sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "        sharepoint_dir = Path(user_dir / gm_dir / gm_docs)\n",
    "\n",
    "        # download_dir = os.path.join(sharepoint_dir, 'Data Shuttle', 'downloads')\n",
    "        download_dir = Path(sharepoint_dir / 'Data Shuttle' / 'downloads')\n",
    "\n",
    "    return sharepoint_dir, download_dir, user_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the folder defaults look for the files we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(download_dir):\n",
    "    # find any changed files changed in past 2hrs in the downloads directory\n",
    "    dirpath = download_dir\n",
    "    files = []\n",
    "    for p, ds, fs in os.walk(dirpath):\n",
    "        for fn in fs:\n",
    "            if 'Updated_' in fn:\n",
    "                # was using this to filter what filenames to find\n",
    "                filepath = os.path.join(p, fn)\n",
    "                files.append(filepath)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Write to excel\n",
    "\n",
    "Call xlwings with your pre-prepared dictionary and write out many sheets to one excel file, naming the sheets whatever you called your dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl(outfile, df_dict):\n",
    "    import xlwings as xw\n",
    "    with xw.App(visible=True) as app:\n",
    "        try:\n",
    "            wb = xw.Book(outfile)\n",
    "            print (\"writing to existing {}\".format(outfile))\n",
    "        except FileNotFoundError:\n",
    "            # create a new book\n",
    "            print (\"creating new {}\".format(outfile))\n",
    "            wb = xw.Book()\n",
    "            wb.save(outfile)\n",
    "\n",
    "        for key in df_dict.keys():\n",
    "            try:\n",
    "                ws = wb.sheets.add(key)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "            \n",
    "            ws = wb.sheets[key]\n",
    "\n",
    "            table_name = key\n",
    "\n",
    "            ws.clear()\n",
    "\n",
    "            df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "            if table_name in [table.df for table in ws.tables]:\n",
    "                ws.tables[table_name].update(df)\n",
    "            else:\n",
    "                table_name = ws.tables.add(source=ws['A1'],\n",
    "                                            name=table_name).update(df)\n",
    "    wb.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out to excel using sub system\n",
    "\n",
    "This was used previously (GMD) might be useful again so haven't removed, but not currently calling.\n",
    "\n",
    "Writes out the checks to sheets filtered against the sub system - maybe useful if we wanted to give the problem rows to a team to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl_sub_system(dict_checks):\n",
    "    sub_sys = dict_checks[check]['Sub System'].unique()\n",
    "    sub_sys.sort()\n",
    "\n",
    "    for s_sys in sub_sys:\n",
    "\n",
    "        df_temp = dict_checks[check][dict_checks[check]['Sub System'] == s_sys]\n",
    "\n",
    "        if df_temp.shape[0] > 0:\n",
    "            df_temp.to_excel(writer, sheet_name=s_sys, index=False)\n",
    "\n",
    "            ws = writer.sheets[s_sys]\n",
    "            wb = writer.book\n",
    "\n",
    "            excel_formatting.adjust_col_width_from_col(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main Processing\n",
    "\n",
    "This is where the processing begins, and where we call the functions defined above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIH_POA\n",
      "BOP_FIP\n",
      "FAS_FAS\n",
      "FIP_FIP\n",
      "FIP_FAS\n",
      "Non_MIH_AIH_Level_4\n",
      "FAS_Wrong_Parent_Source_code\n",
      "FAS_as_BOF\n",
      "make_no_buy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # for reading in multiple files\n",
    "\n",
    "    # files = find_files()\n",
    "    dict_df = {}\n",
    "\n",
    "    filename = 'Updated_T48e-01-Z00001_2024-07-19.xlsx'\n",
    "    sharepoint_dir, download_dir, user_dir = set_folder_defaults()\n",
    "\n",
    "    file = Path(download_dir) / filename\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        # reading in the historic excel files\n",
    "        df = pd.read_excel(f, parse_dates=True)\n",
    "        f.close()\n",
    "\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.rename(columns={'index':'bom_order'}, inplace=True)\n",
    "\n",
    "    # add parent source code to each row for validation checks to come\n",
    "    df = parent_source_code(df)\n",
    "\n",
    "    # initialise a dictionary to store all the check output as dataframes\n",
    "    dict_checks = {}\n",
    "\n",
    "    # complete the source code with parent source code checks\n",
    "    dict_checks, df = source_code_within_parent_checks(dict_checks, df)\n",
    "\n",
    "    # check all level 4 have the correct source code\n",
    "    dict_checks, df = check_level_4_source_code_checks(dict_checks, df)\n",
    "\n",
    "    # check for FAS with the wrong source code\n",
    "    dict_checks, df = FAS_wrong_parent_source_code(dict_checks, df)\n",
    "\n",
    "    # complete the fasteners source code checks\n",
    "    dict_checks, df = fastener_checks(dict_checks, df)\n",
    "\n",
    "    # look for make assemblys with no parts to buy\n",
    "    df, make_no_buy = check_make_no_buy(df)\n",
    "    dict_checks['make_no_buy'] = df.loc[make_no_buy]\n",
    "\n",
    "    # write out just the cols we need to report against\n",
    "    dict_checks = filter_check_columns(dict_checks)\n",
    "\n",
    "    # add the full df to the sheet\n",
    "    dict_checks['BOM'] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Development\n",
    "\n",
    "Dumping ground for checks that might come in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Level 4 ASSY\n",
    "\n",
    "Should any part with ASSY in description be at Level 4 only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level\n",
       "4.0     382\n",
       "5.0     688\n",
       "6.0     444\n",
       "7.0     405\n",
       "8.0     165\n",
       "9.0      54\n",
       "10.0      6\n",
       "11.0     21\n",
       "12.0      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Description.str.contains('ASSY', na=False)].groupby(['Level']).size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Source Codes\n",
    "\n",
    "Check whether a part has been configured with more than one source code within the product structure\n",
    "\n",
    "Is it valid to have a TFF part that's FAS and POA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_source_code(df):\n",
    "    unstacked = df.groupby(['Title','Revision','Source Code']).size().unstack()\n",
    "\n",
    "    # find number of columns dynamically, as number of unique status controls the number of columns\n",
    "    expected_status_count = len(unstacked.columns) - 1\n",
    "    unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_status_count]\n",
    "    unstacked2\n",
    "\n",
    "\n",
    "    multi_sc = unstacked2.reset_index().fillna('')\n",
    "\n",
    "    # make_sc_cols = ['AIH','MIH','MOB']\n",
    "\n",
    "    first_cols = ['Title', 'Revision']\n",
    "\n",
    "    cols_to_order = first_cols\n",
    "    sc_ordered_cols = cols_to_order + (multi_sc.columns.drop(cols_to_order).tolist())\n",
    "\n",
    "    multi_sc = multi_sc[sc_ordered_cols]\n",
    "\n",
    "    return multi_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Source Code</th>\n",
       "      <th>Title</th>\n",
       "      <th>Revision</th>\n",
       "      <th>AIH</th>\n",
       "      <th>BOF</th>\n",
       "      <th>BOP</th>\n",
       "      <th>CON</th>\n",
       "      <th>ENG</th>\n",
       "      <th>FAS</th>\n",
       "      <th>FIP</th>\n",
       "      <th>FLA</th>\n",
       "      <th>POA</th>\n",
       "      <th>SOF</th>\n",
       "      <th>SYS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFF-SA380</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Source Code      Title  Revision AIH BOF BOP CON ENG  FAS FIP FLA  POA SOF SYS\n",
       "0            TFF-SA380       1.0                      3.0          1.0        "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_sc = multi_source_code(df)\n",
    "multi_sc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out source code checks to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to existing /Users/mark/Documents/BOM_from_3DX/Updated_T48e-01-Z00001_2024-07-19_validated.xlsx\n",
      "Sheet named 'AIH_POA' already present in workbook\n",
      "Sheet named 'BOP_FIP' already present in workbook\n",
      "Sheet named 'FAS_FAS' already present in workbook\n",
      "Sheet named 'FIP_FIP' already present in workbook\n",
      "Sheet named 'FIP_FAS' already present in workbook\n",
      "Sheet named 'Non_MIH_AIH_Level_4' already present in workbook\n",
      "Sheet named 'FAS_Wrong_Parent_Source_code' already present in workbook\n",
      "Sheet named 'FAS_as_BOF' already present in workbook\n",
      "Sheet named 'make_no_buy' already present in workbook\n",
      "Sheet named 'BOM' already present in workbook\n"
     ]
    }
   ],
   "source": [
    "# Write out to excel\n",
    "pathfile = Path(file.name).stem\n",
    "output_file = Path(sys.path[0]) / Path(pathfile + '_validated').with_suffix('.xlsx')\n",
    "# write_to_xl(output_file, dict_checks)\n",
    "\n",
    "# using inline write to excel as this seems to work better on mac.  \n",
    "outfile = output_file\n",
    "df_dict = dict_checks\n",
    "\n",
    "import xlwings as xw\n",
    "try:\n",
    "    wb = xw.Book(output_file)\n",
    "    print (\"writing to existing {}\".format(outfile))\n",
    "except FileNotFoundError:\n",
    "    # create a new book\n",
    "    print (\"creating new {}\".format(outfile))\n",
    "    wb = xw.Book()\n",
    "    wb.save(outfile)\n",
    "\n",
    "for key in df_dict.keys():\n",
    "    try:\n",
    "        ws = wb.sheets.add(key)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    \n",
    "    ws = wb.sheets[key]\n",
    "\n",
    "    table_name = key\n",
    "\n",
    "    ws.clear()\n",
    "\n",
    "    df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "    if len(df) > 0:\n",
    "        if table_name in [table.df for table in ws.tables]:\n",
    "            ws.tables[table_name].update(df)\n",
    "        else:\n",
    "            table_name = ws.tables.add(source=ws['A1'],\n",
    "                                        name=table_name).update(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
