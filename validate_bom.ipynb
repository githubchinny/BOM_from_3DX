{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate \n",
    "Author: Mark Chinnock  \n",
    "Date: 19/07/2024\n",
    "\n",
    "This script can read in an excel file(s) from sharepoint that have previously been extracted from 3dx, processed to add the additional functions/metrics and passed to smartsheets.\n",
    "\n",
    "Alternatively, this could read in the 3dx extract directly to validate the latest attributes.\n",
    "\n",
    "It can process as many files as you want, for as many product structures as you want, and build a history of data quality.  Currently writing out to excel file named the same as the input file and suffixed with '_validated'.\n",
    "\n",
    "Required Inputs:\n",
    "* 3dx extract xlsx file - as long as the standard columns used for calculating the metrics are present this will process it.\n",
    "\n",
    "Outputs Produced:\n",
    "* xlsx spreadsheet with full input BOM written to a sheet with columns for each metric appended to far right columns for reporting in power bi/excel\n",
    "* Additional sheets written into xlsx for each validation rule, showing the subset of records that failed each rule (might be useful for quick dashboard displays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Makes without Buys\n",
    "For each Make part there should be an assembly that needs at least one Buy part below it to make sense - if you're going to bake a cake, you need at least 1 ingredient!  If you're buying a cake, then you don't need anything else!\n",
    "\n",
    "If a MAKE (source code 'AIH','MIH','MOB') is not followed by a child BUY this is a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level'])].index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    df['make_no_buy'] = np.where((df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level']), True, False)\n",
    "\n",
    "    return df, make_no_buy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parent Source Code\n",
    "\n",
    "We will need to track the source code of the parent part and use it for validate checks coming later.\n",
    "\n",
    "This interates through the dataframe and appends to each row the parent part source code (the level numerically above this row's level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_source_code(df):\n",
    "    prev_level = 0\n",
    "\n",
    "    level_source = {}\n",
    "\n",
    "    for i, x in df.iterrows():\n",
    "        # take the current level source and store it\n",
    "        level_source[x['Level']] = x['Source Code']\n",
    "        if ((x['Level'] >= 4)):\n",
    "            df.loc[i, 'Parent Source Code'] = level_source[x['Level'] - 1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Source Code within parent checks\n",
    "\n",
    "sc_check_list is a list of invalid scenarios we are checking for with syntax: SOURCE CODE_PARENT SOURCE CODE\n",
    "\n",
    "A dataframe of invalid rows is written to dict_checks[sc_check] and a column is added to the end of the main dataframe with the sc_check as a column name holding true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_code_within_parent_checks(dict_checks, df):\n",
    "    # check for combinations of source codes within a parent source that's not accepted\n",
    "    sc_check_list = ['AIH_POA','BOP_FIP','FAS_FAS','FIP_FIP','FIP_FAS']\n",
    "    \n",
    "    for sc_check in sc_check_list:\n",
    "        sc, parent_sc = sc_check.split('_')\n",
    "\n",
    "        dict_checks[sc_check] = df[(df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc)]\n",
    "\n",
    "        df[sc_check] = np.where((df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc), True, False)\n",
    "\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Level 4 Source Code Checks\n",
    "\n",
    "level 4 (assembly level when first level = 0) should only have Source Code 'MIH' or 'AIH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_level_4_source_code_checks(dict_checks, df):\n",
    "    # level 4 can only be MIH or AIH\n",
    "    dict_checks['Non_MIH_AIH_Level_4'] = df[(df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH']))]\n",
    "    df['Non_MIH_AIH_Level_4'] = np.where((df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Fasteners with wrong parent source code\n",
    "\n",
    "Fasteners should only be within parents of 'FIP','AIH,'MIH'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAS_wrong_parent_source_code(dict_checks, df):\n",
    "    # FAS can only be within a FIP, AIH or MIH parent\n",
    "    dict_checks['FAS_Wrong_Parent_Source_code'] = df[(df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH']))]\n",
    "    df['FAS_Wrong_Parent_Source_code'] = np.where((df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Fastener checks\n",
    "\n",
    "Look for scenarios where a description says washer, bolt or grommet but the source code says 'BOF'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastener_checks(dict_checks, df):\n",
    "    # All BOF records that are fasteners should be {FAS}teners in the BOMS\n",
    "    # Part Description contains washer, bolt, grommet\n",
    "    # Source code = \"BOF\"\n",
    "    fastener_check_list = ['^washer|^bolt|^grommet']        \n",
    "\n",
    "    dict_checks['FAS_as_BOF'] = df[(df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF')]\n",
    "    df['FAS_as_BOF'] = np.where((df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Filter check columns\n",
    "\n",
    "For writing out to excel on separate sheets, only need to keep the pertinent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_check_columns(dict_checks):\n",
    "    # reduce the selection of columns used for writing out later\n",
    "    check_columns = [\n",
    "    'orig_sort',\n",
    "    'Last Modified By',\n",
    "    'Owner',\n",
    "    'Function Group',\n",
    "    'System',\n",
    "    'Sub System',\n",
    "    'Level',\n",
    "    'Title',\n",
    "    'Revision',\n",
    "    'Description',\n",
    "    'Parent Part',\n",
    "    'Source Code',\n",
    "    'Quantity',\n",
    "    'Parent Source Code'\n",
    "    ]\n",
    "\n",
    "    for key in dict_checks.keys():\n",
    "        print (key)\n",
    "        dict_checks[key] = dict_checks[key][check_columns]\n",
    "\n",
    "    return dict_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Validate GMT Standards 3DX Attributes Requirements\n",
    "\n",
    "Read in the GMT Standards document from GMT sharepoint folder and confirm 3dx extract contains the same columns and valid values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_attributes(df):\n",
    "    attr_filename = '3DX Attributes Requirements for Release and Clarification.xlsx'\n",
    "\n",
    "    attr = pd.read_excel(attr_filename, sheet_name='Drop Down Attributes', na_values=\"\", keep_default_na=False)\n",
    "\n",
    "    # create a dictionary of all the valid values for each column - this drops the nan values for each column\n",
    "    attr_d = {attr[column].name: [y for y in attr[column] if not pd.isna(y)] for column in attr}\n",
    "\n",
    "    for key in attr_d:\n",
    "        # check the column exists\n",
    "        try:\n",
    "            mask = df[key].isin(attr_d[key])\n",
    "            df[key + ' Check'] = np.where(mask, 'Valid','Invalid')\n",
    "        except KeyError as e:\n",
    "            df[key + ' Check'] = 'Not in Extract'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.9 Validate BoM and Function Group Structure GMT document\n",
    "\n",
    "This document is stored on GMT-EngineeringBoM sharepoint in GMT Standards folder:  \n",
    "https://forsevengroup.sharepoint.com/:x:/r/sites/GMT-EngineeringBoM/Shared%20Documents/GMT%20-%20Standards/BoM%20and%20Function%20Group%20Structure%20GMT.xlsx?d=wc3cbfc77631c40b69ba7d5026066a2e7&csf=1&web=1&e=B64OP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10 Validate Part No\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_part_no(df):\n",
    "\n",
    "    # ? means the preceding bracketed group is optional (optional s,S and trailing X)\n",
    "    # pattern = r'([A-Z]\\d{2}([s,S])?-[A-Z]\\d{4}(X)?)'\n",
    "    # pattern = r'[A-Z]\\d{2}[e]-[A-Z]\\d{5}+X?'\n",
    "    # pattern = r'([A-Z]\\d{2}[e])-([A-Z])(\\d{5})(X)'\n",
    "    pattern = r'([A-Z]\\d{2}[e])-(\\w[A-Za-z0-9]*)?-?([A-Z])(\\d{5})(X)?'\n",
    "    df[['extr_project','extr_invalid_code','extr_function','extr_pn','extr_maturity']] = df['Title'].str.extract(pattern, expand=True)\n",
    "\n",
    "    # done outside of this function now\n",
    "    # df['part_number_length'] = df['Title'].str.len()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Script config setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import xlwings as xw\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import platform\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to determine whether we're running in Juypter notebook or as a command line script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_metrics(df, product):\n",
    "    for col in ['UOM','Provide','Source Code']:\n",
    "        df['Missing {}'.format(col)] = np.where(df[col].isnull(), 1, 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAD_Material_validation(df):\n",
    "    df[1:][df['Title'].str.contains('TPP', na=False)].groupby(['Title','CAD Material']).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bi_key(df):\n",
    "    # for use in power_bi reporting\n",
    "    # replace NaN with ''\n",
    "    df['bi_combined_key'] = df['Variant'].astype(str) + df['Function Group'].astype(str) + df['System'].astype(str) + df['Sub System'].astype(str)\n",
    "    \n",
    "    return df['bi_combined_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine the folder structure based on whether we're running on a test windows pc, in azure server, a mac, or in the real world against sharepoint - helps Mark test on different devices! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_folder_defaults():\n",
    "    if 'macOS' in platform.platform():\n",
    "        # set some defaults for testing on mac\n",
    "        download_dir = Path('/Users/mark/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir\n",
    "\n",
    "    elif 'Server' in platform.platform():\n",
    "        # we're on the azure server (probably)\n",
    "        user_dir = Path('Z:/python/FilesIn')\n",
    "\n",
    "        download_dir = Path(user_dir)\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = Path('Z:/python/FilesOut')\n",
    "\n",
    "    elif os.getlogin() == 'mark_':\n",
    "        # my test windows machine\n",
    "        download_dir = Path('C:/Users/mark_/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir        \n",
    "\n",
    "    else:\n",
    "        # personal one drive\n",
    "        user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "        # replace USERNAME with current logged on user\n",
    "        user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "        # read in config file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('user_directory.ini')\n",
    "\n",
    "        # read in gm_dir and gm_docs from config file\n",
    "        gm_dir = Path(config[os.getlogin().lower()]['gm_dir'])\n",
    "        gm_docs = Path(config[os.getlogin().lower()]['gmt'])\n",
    "        # this may find more than one sharepoint directory\n",
    "        # sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "        sharepoint_dir = Path(user_dir / gm_dir / gm_docs)\n",
    "\n",
    "        # download_dir = os.path.join(sharepoint_dir, 'Data Shuttle', 'downloads')\n",
    "        download_dir = Path(sharepoint_dir / 'Data Shuttle' / 'downloads')\n",
    "\n",
    "    return sharepoint_dir, download_dir, user_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the folder defaults look for the files we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(download_dir):\n",
    "    # find any changed files changed in past 2hrs in the downloads directory\n",
    "    dirpath = download_dir\n",
    "    files = []\n",
    "    for p, ds, fs in os.walk(dirpath):\n",
    "        for fn in fs:\n",
    "            if 'Updated_' in fn:\n",
    "                # was using this to filter what filenames to find\n",
    "                filepath = os.path.join(p, fn)\n",
    "                files.append(filepath)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Write to excel\n",
    "\n",
    "Call xlwings with your pre-prepared dictionary and write out many sheets to one excel file, naming the sheets whatever you called your dictionary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl(outfile, df_dict):\n",
    "    import xlwings as xw\n",
    "    with xw.App(visible=True) as app:\n",
    "        try:\n",
    "            wb = xw.Book(outfile)\n",
    "            print (\"writing to existing {}\".format(outfile))\n",
    "        except FileNotFoundError:\n",
    "            # create a new book\n",
    "            print (\"creating new {}\".format(outfile))\n",
    "            wb = xw.Book()\n",
    "            wb.save(outfile)\n",
    "\n",
    "        for key in df_dict.keys():\n",
    "            try:\n",
    "                ws = wb.sheets.add(key)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "            \n",
    "            ws = wb.sheets[key]\n",
    "\n",
    "            table_name = key\n",
    "\n",
    "            ws.clear()\n",
    "\n",
    "            df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "            if table_name in [table.df for table in ws.tables]:\n",
    "                ws.tables[table_name].update(df)\n",
    "            else:\n",
    "                table_name = ws.tables.add(source=ws['A1'],\n",
    "                                            name=table_name).update(df)\n",
    "    wb.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out to excel using sub system\n",
    "\n",
    "This was used previously (GMD) might be useful again so haven't removed, but not currently calling.\n",
    "\n",
    "Writes out the checks to sheets filtered against the sub system - maybe useful if we wanted to give the problem rows to a team to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl_sub_system(dict_checks):\n",
    "    sub_sys = dict_checks[check]['Sub System'].unique()\n",
    "    sub_sys.sort()\n",
    "\n",
    "    for s_sys in sub_sys:\n",
    "\n",
    "        df_temp = dict_checks[check][dict_checks[check]['Sub System'] == s_sys]\n",
    "\n",
    "        if df_temp.shape[0] > 0:\n",
    "            df_temp.to_excel(writer, sheet_name=s_sys, index=False)\n",
    "\n",
    "            ws = writer.sheets[s_sys]\n",
    "            wb = writer.book\n",
    "\n",
    "            excel_formatting.adjust_col_width_from_col(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main Processing\n",
    "\n",
    "This is where the processing begins, and where we call the functions defined above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # for reading in multiple files\n",
    "\n",
    "    # files = find_files()\n",
    "    dict_df = {}\n",
    "\n",
    "    filename = 'Updated_T48e-01-Z00001_2024-07-25.xlsx'\n",
    "\n",
    "    product = re.split('_|-', filename)[1].upper()\n",
    "\n",
    "    sharepoint_dir, download_dir, user_dir = set_folder_defaults()\n",
    "\n",
    "    file = Path(download_dir) / filename\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        # reading in the historic excel files\n",
    "        df = pd.read_excel(f, parse_dates=True)\n",
    "        f.close()\n",
    "\n",
    "    # copy df without the 1st row of metrics and 1st col of BOM COUNTs\n",
    "    df = df.iloc[1:,1:]        \n",
    "\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.rename(columns={'index':'bom_order'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIH_POA\n",
      "BOP_FIP\n",
      "FAS_FAS\n",
      "FIP_FIP\n",
      "FIP_FAS\n",
      "Non_MIH_AIH_Level_4\n",
      "FAS_Wrong_Parent_Source_code\n",
      "FAS_as_BOF\n",
      "make_no_buy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # add variant column for merging multiple BOMs together and reporting on 1 dashboard\n",
    "    df['Variant'] = product\n",
    "    df['bi_combined_key'] = add_bi_key(df)    \n",
    "\n",
    "    df = add_missing_metrics(df, product)\n",
    "    # add part validation\n",
    "    df = validate_part_no(df)\n",
    "\n",
    "    # add parent source code to each row for validation checks to come\n",
    "    df = parent_source_code(df)\n",
    "\n",
    "    # initialise a dictionary to store all the check output as dataframes\n",
    "    dict_checks = {}\n",
    "\n",
    "    # complete the source code with parent source code checks\n",
    "    dict_checks, df = source_code_within_parent_checks(dict_checks, df)\n",
    "\n",
    "    # check all level 4 have the correct source code\n",
    "    dict_checks, df = check_level_4_source_code_checks(dict_checks, df)\n",
    "\n",
    "    # check for FAS with the wrong source code\n",
    "    dict_checks, df = FAS_wrong_parent_source_code(dict_checks, df)\n",
    "\n",
    "    # complete the fasteners source code checks\n",
    "    dict_checks, df = fastener_checks(dict_checks, df)\n",
    "\n",
    "    # look for make assemblys with no parts to buy\n",
    "    df, make_no_buy = check_make_no_buy(df)\n",
    "    dict_checks['make_no_buy'] = df.loc[make_no_buy]\n",
    "\n",
    "    # validate the 3dx attributes that have dropdowns\n",
    "    df = check_attributes(df)\n",
    "\n",
    "    # write out just the cols we need to report against\n",
    "    dict_checks = filter_check_columns(dict_checks)\n",
    "\n",
    "    # add the full df to the sheet\n",
    "    dict_checks['BOM'] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Development\n",
    "\n",
    "Dumping ground for checks that might come in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Level 4 ASSY\n",
    "\n",
    "Should any part with ASSY in description be at Level 4 only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Non_level_4_ASSY(df):\n",
    "    df[df.Description.str.contains('ASSY', na=False)].groupby(['Level']).size()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Source Codes\n",
    "\n",
    "Check whether a part has been configured with more than one source code within the product structure\n",
    "\n",
    "Is it valid to have a TFF part that's FAS and POA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_source_code(df):\n",
    "    unstacked = df.groupby(['Title','Revision','Source Code']).size().unstack()\n",
    "\n",
    "    # find number of columns dynamically, as number of unique status controls the number of columns\n",
    "    expected_status_count = len(unstacked.columns) - 1\n",
    "    unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_status_count]\n",
    "    unstacked2\n",
    "\n",
    "\n",
    "    multi_sc = unstacked2.reset_index().fillna('')\n",
    "\n",
    "    # make_sc_cols = ['AIH','MIH','MOB']\n",
    "\n",
    "    first_cols = ['Title', 'Revision']\n",
    "\n",
    "    cols_to_order = first_cols\n",
    "    sc_ordered_cols = cols_to_order + (multi_sc.columns.drop(cols_to_order).tolist())\n",
    "\n",
    "    multi_sc = multi_sc[sc_ordered_cols]\n",
    "\n",
    "    return multi_sc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out source code checks to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to existing c:\\Users\\mark_\\Documents\\GitHub\\BOM_from_3DX\\Updated_T48e-01-Z00001_2024-07-25_validated.xlsx\n",
      "Sheet named 'AIH_POA' already present in workbook\n",
      "Sheet named 'BOP_FIP' already present in workbook\n",
      "Sheet named 'FAS_FAS' already present in workbook\n",
      "Sheet named 'FIP_FIP' already present in workbook\n",
      "Sheet named 'FIP_FAS' already present in workbook\n",
      "Sheet named 'Non_MIH_AIH_Level_4' already present in workbook\n",
      "Sheet named 'FAS_Wrong_Parent_Source_code' already present in workbook\n",
      "Sheet named 'FAS_as_BOF' already present in workbook\n",
      "Sheet named 'make_no_buy' already present in workbook\n",
      "Sheet named 'BOM' already present in workbook\n"
     ]
    }
   ],
   "source": [
    "# Write out to excel\n",
    "pathfile = Path(file.name).stem\n",
    "output_file = Path(sys.path[0]) / Path(pathfile + '_validated').with_suffix('.xlsx')\n",
    "# write_to_xl(output_file, dict_checks)\n",
    "\n",
    "# using inline write to excel as this seems to work better on mac.  \n",
    "outfile = output_file\n",
    "df_dict = dict_checks\n",
    "\n",
    "import xlwings as xw\n",
    "try:\n",
    "    wb = xw.Book(output_file)\n",
    "    print (\"writing to existing {}\".format(outfile))\n",
    "except FileNotFoundError:\n",
    "    # create a new book\n",
    "    print (\"creating new {}\".format(outfile))\n",
    "    wb = xw.Book()\n",
    "    wb.save(outfile)\n",
    "\n",
    "for key in df_dict.keys():\n",
    "    try:\n",
    "        ws = wb.sheets.add(key)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    \n",
    "    ws = wb.sheets[key]\n",
    "\n",
    "    table_name = key\n",
    "\n",
    "    ws.clear()\n",
    "\n",
    "    df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "    if len(df) > 0:\n",
    "        if table_name in [table.df for table in ws.tables]:\n",
    "            ws.tables[table_name].update(df)\n",
    "        else:\n",
    "            table_name = ws.tables.add(source=ws['A1'],\n",
    "                                        name=table_name).update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_filename = 'BoM and Function Group Structure GMT.xlsx'\n",
    "\n",
    "struct = pd.read_excel(struct_filename, sheet_name='T48E')\n",
    "# drop first row of struct which should be project, model variant, function group area, systems, sub sytems, AMs\n",
    "struct = struct.loc[1:]\n",
    "\n",
    "# find the first nan row in Level 4s as this will show where the last row in the valid values ends\n",
    "last_row = struct['Level 4'].isna().idxmax()-1\n",
    "\n",
    "# drop the rest of the struct rows\n",
    "struct = struct.loc[:last_row]\n",
    "\n",
    "# and now create a dictionary of all the valid values for each column - this drops the nan values for each column where we read merged cells from excel\n",
    "struct_d = {struct[column].name: [y for y in struct[column] if not pd.isna(y)] for column in struct}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Replacement index 0 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lvl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(struct_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLevel \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lvl)])) \u001b[38;5;241m!=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLevel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mlvl\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction Group Area mismatch.  Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat())\n",
      "\u001b[1;31mIndexError\u001b[0m: Replacement index 0 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "for lvl in range(0,5):\n",
    "    if len(list(struct_d['Level {}'.format(lvl)])) != df[df['Level']==lvl+1].shape[0]:\n",
    "        print (\"Function Group Area mismatch.  Expected {} but got {}\".format())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Function/System Group mismatch.  Expected 24 but got 22\n",
      "Level 3: Function/System Group mismatch.  Expected 81 but got 75\n",
      "Level 4: Function/System Group mismatch.  Expected 21 but got 367\n"
     ]
    }
   ],
   "source": [
    "for lvl in range(0,5):\n",
    "    expected = len(struct_d['Level {}'.format(lvl+1)])\n",
    "    got = df[df['Level']==lvl].shape[0]\n",
    "    if  expected != got:\n",
    "        print (\"Level {}: Function/System Group mismatch.  Expected {} but got {}\".format(lvl, expected, got))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bom_order</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T48E-01-A01</td>\n",
       "      <td>STRUCTURE SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>T48E-01-B01</td>\n",
       "      <td>FRONT AXLE-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>T48E-01-B02</td>\n",
       "      <td>REAR AXLE-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>T48E-01-B03</td>\n",
       "      <td>WHEELS &amp; TYRES-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>T48E-01-C01</td>\n",
       "      <td>BRAKE ACTUATION SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>T48E-01-D01</td>\n",
       "      <td>STEERING SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>T48E-01-E01</td>\n",
       "      <td>AIR SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>T48E-01-F01</td>\n",
       "      <td>ELECTRIC POWER SYSTEMS-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>T48E-01-G01</td>\n",
       "      <td>ELECTRIC DRIVE SYSTEMS-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>T48E-01-J01</td>\n",
       "      <td>POWERTRAIN NVH &amp; HEATSHIELDS-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>T48E-01-L01</td>\n",
       "      <td>THERMAL SYSTEMS-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>T48E-M01</td>\n",
       "      <td>CONTROL SYSTEMS - 5 DOOR - XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>T48E-M02</td>\n",
       "      <td>POWER SYSTEMS - 5 DOOR - XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>T48E-M04</td>\n",
       "      <td>ELECTRICAL DISTRIBUTION SYSTEMS - 5 DOOR - XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>T48E-M05</td>\n",
       "      <td>MULTIMEDIA SYSTEMS - 5 DOOR - XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>T48E-M06</td>\n",
       "      <td>SAFETY &amp; SECURITY SYSTEMS - 5 DOOR - XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>T48E-M07</td>\n",
       "      <td>SOFTWARE SYSTEMS - 5 DOOR - XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>T48E-01-N01</td>\n",
       "      <td>INTERIOR SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>T48E-01-N02</td>\n",
       "      <td>HVAC SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>T48E-01-V01</td>\n",
       "      <td>EXTERIOR ACCESSORIES-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>T48E-01-A02</td>\n",
       "      <td>PANELS &amp; CLOSURE SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>T48E-01-A03</td>\n",
       "      <td>EXTERIOR SYSTEMS-5 DOOR-XP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title                                    Description\n",
       "bom_order                                                            \n",
       "3          T48E-01-A01                    STRUCTURE SYSTEMS-5 DOOR-XP\n",
       "698        T48E-01-B01                           FRONT AXLE-5 DOOR-XP\n",
       "863        T48E-01-B02                            REAR AXLE-5 DOOR-XP\n",
       "1033       T48E-01-B03                       WHEELS & TYRES-5 DOOR-XP\n",
       "1157       T48E-01-C01              BRAKE ACTUATION SYSTEMS-5 DOOR-XP\n",
       "1238       T48E-01-D01                     STEERING SYSTEMS-5 DOOR-XP\n",
       "1268       T48E-01-E01                          AIR SYSTEMS-5 DOOR-XP\n",
       "1302       T48E-01-F01                      ELECTRIC POWER SYSTEMS-XP\n",
       "1312       T48E-01-G01                      ELECTRIC DRIVE SYSTEMS-XP\n",
       "1333       T48E-01-J01                POWERTRAIN NVH & HEATSHIELDS-XP\n",
       "1453       T48E-01-L01                             THERMAL SYSTEMS-XP\n",
       "1693          T48E-M01                  CONTROL SYSTEMS - 5 DOOR - XP\n",
       "1966          T48E-M02                    POWER SYSTEMS - 5 DOOR - XP\n",
       "1991          T48E-M04  ELECTRICAL DISTRIBUTION SYSTEMS - 5 DOOR - XP\n",
       "2545          T48E-M05               MULTIMEDIA SYSTEMS - 5 DOOR - XP\n",
       "2574          T48E-M06        SAFETY & SECURITY SYSTEMS - 5 DOOR - XP\n",
       "2581          T48E-M07                 SOFTWARE SYSTEMS - 5 DOOR - XP\n",
       "2585       T48E-01-N01                     INTERIOR SYSTEMS-5 DOOR-XP\n",
       "2695       T48E-01-N02                         HVAC SYSTEMS-5 DOOR-XP\n",
       "2714       T48E-01-V01                        EXTERIOR ACCESSORIES-XP\n",
       "2734       T48E-01-A02             PANELS & CLOSURE SYSTEMS-5 DOOR-XP\n",
       "3294       T48E-01-A03                     EXTERIOR SYSTEMS-5 DOOR-XP"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Level']==2][['Title','Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
