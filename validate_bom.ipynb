{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate BoM\n",
    "\n",
    "The notebook/script reads in the df file from sharepoint based on the existing live BoM for the chosen project.  It will correct the part numbers being used by matching with catia   \n",
    "and update the release status to what's found in the release registers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import xlwings as xw\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import platform\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    '''\n",
    "        determine where this script is running\n",
    "        return either jupyter, ipython, terminal\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "        if 'terminal' in ipy_str:\n",
    "            return 'ipython'\n",
    "    except:\n",
    "        return 'terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files():\n",
    "    if 'macOS' in platform.platform():\n",
    "        # set some defaults for testing on mac\n",
    "        download_dir = Path('/Users/mark/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir\n",
    "\n",
    "    elif 'Server' in platform.platform():\n",
    "        # we're on the azure server (probably)\n",
    "        user_dir = Path('Z:/python/FilesIn')\n",
    "\n",
    "        download_dir = Path(user_dir)\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = Path('Z:/python/FilesOut')\n",
    "\n",
    "    elif os.getlogin() == 'mark_':\n",
    "        # my test windows machine\n",
    "        download_dir = Path('C:/Users/mark_/Downloads')\n",
    "        user_dir = download_dir\n",
    "        sharepoint_dir = download_dir        \n",
    "\n",
    "    else:\n",
    "        # personal one drive\n",
    "        user_dir = 'C:/Users/USERNAME'\n",
    "\n",
    "        # replace USERNAME with current logged on user\n",
    "        user_dir = user_dir.replace('USERNAME', os.getlogin())\n",
    "\n",
    "        # read in config file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('user_directory.ini')\n",
    "\n",
    "        # read in gm_dir and gm_docs from config file\n",
    "        gm_dir = Path(config[os.getlogin().lower()]['gm_dir'])\n",
    "        gm_docs = Path(config[os.getlogin().lower()]['gmt'])\n",
    "        # this may find more than one sharepoint directory\n",
    "        # sharepoint_dir = user_dir + \"/\" + gm_dir + \"/\" + gm_docs\n",
    "        sharepoint_dir = Path(user_dir / gm_dir / gm_docs)\n",
    "\n",
    "        # download_dir = os.path.join(sharepoint_dir, 'Data Shuttle', 'downloads')\n",
    "        download_dir = Path(sharepoint_dir / 'Data Shuttle' / 'downloads')\n",
    "\n",
    "    # find any changed files changed in past 2hrs in the downloads directory\n",
    "    dirpath = download_dir\n",
    "    files = []\n",
    "    for p, ds, fs in os.walk(dirpath):\n",
    "        for fn in fs:\n",
    "            if 'Updated_' in fn:\n",
    "                # was using this to filter what filenames to find\n",
    "                filepath = os.path.join(p, fn)\n",
    "                files.append(filepath)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df, col_name):\n",
    "    cols = df.columns\n",
    "\n",
    "    # filter out all Unnamed cols\n",
    "    cols = [ x for x in cols if col_name not in x ]\n",
    "\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makes without Buys\n",
    "Each Make part needs at least one Buy part below it to make sense\n",
    "\n",
    "If a MAKE is not followed by a child BUY this is a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_make_no_buy(df):\n",
    "    # if MAKE and there is no BUY below next row's part level less than or equal to current part level we have a MAKE without a BUY\n",
    "    # df['PROVIDE'] = np.where(df['Source Code'].isin(['AIH','MIH','MOB']),'Make','Buy')\n",
    "    make_no_buy = list(df[(df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level'])].index)\n",
    "    make_no_buy = sorted(make_no_buy)\n",
    "    df['make_no_buy'] = np.where((df['Source Code'].isin(['AIH','MIH','MOB'])) & (df['Level'].shift(-1) <= df['Level']), True, False)\n",
    "\n",
    "    return df, make_no_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_source_code(df):\n",
    "    prev_level = 0\n",
    "\n",
    "    level_source = {}\n",
    "\n",
    "    for i, x in df.iterrows():\n",
    "        # take the current level source and store it\n",
    "        level_source[x['Level']] = x['Source Code']\n",
    "        if ((x['Level'] >= 4)):\n",
    "            df.loc[i, 'Parent Source Code'] = level_source[x['Level'] - 1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_code_checks(df, sc_check_list):\n",
    "\n",
    "    dict_checks = {}\n",
    "\n",
    "    fastener_list = ['^WASHER|^BOLT|^GROMMET']\n",
    "    for sc_check in sc_check_list:\n",
    "        sc, parent_sc = sc_check.split('_')\n",
    "\n",
    "        dict_checks[sc_check] = df[(df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc)]\n",
    "\n",
    "        df[sc_check] = np.where((df['Source Code'] == sc) & (df['Parent Source Code'] == parent_sc), True, False)\n",
    "\n",
    "        # temp_df = df[check_columns][(df['Description'].str.contains('^{}'.format(fastener))) & (df['Source Code'] == 'BOF')]\n",
    "        # df['FAS_as_BOF'] = np.where((df['Description'].str.contains('^{}'.format(fastener))) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "        # dict_checks['FAS_as_BOF'] = pd.concat([dict_checks['Fasteners_as_BOF'], temp_df])\n",
    "\n",
    "    dict_checks['Non_MIH_AIH_Level_4'] = df[(df['Level'] == 4) & (~df['Source Code'].isin(['MIH','AIH']))]\n",
    "    df['FAS_as_BOF'] = np.where((df['Level'] == 4) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    dict_checks['FAS_Wrong_Parent_Source_code'] = df[(df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH']))]\n",
    "    df['FAS_Wrong_Parent_Source_code'] = np.where((df['Source Code'] == 'FAS') & (~df['Parent Source Code'].isin(['FIP','AIH','MIH'])), True, False)\n",
    "\n",
    "    return dict_checks, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastener_checks(dict_checks, df, fastener_check_list):\n",
    "    # All BOF records that are fasteners should be {FAS}teners in the BOMS\n",
    "    # Part Description contains washer, bolt, grommet\n",
    "    # Source code = \"BOF\"\n",
    "        \n",
    "    dict_checks['FAS_as_BOF'] = df[(df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF')]\n",
    "    df['FAS_as_BOF'] = np.where((df['Description'].str.lower().str.contains('{}'.format(fastener_check_list))) & (df['Source Code'] == 'BOF'), True, False)\n",
    "\n",
    "    return dict_checks, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_check_columns(dict_checks):\n",
    "    # For a selection of columns, create a dataframe of AIH parent, POA parts\n",
    "    check_columns = [\n",
    "    # 'row',\n",
    "    'Function Group',\n",
    "    'System',\n",
    "    'Sub System',\n",
    "    'Level',\n",
    "    'Title',\n",
    "    'Revision',\n",
    "    'Description',\n",
    "    'Parent Part',\n",
    "    'Source Code',\n",
    "    'Quantity',\n",
    "    'Parent Source Code'\n",
    "    ]\n",
    "\n",
    "    for key in dict_checks.keys():\n",
    "        print (key)\n",
    "        dict_checks[key] = dict_checks[key][check_columns]\n",
    "\n",
    "    return dict_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl(outfile, df_dict):\n",
    "    import xlwings as xw\n",
    "    with xw.App(visible=False) as app:\n",
    "        try:\n",
    "            wb = xw.Book(outfile)\n",
    "            print (\"writing to existing {}\".format(outfile))\n",
    "        except FileNotFoundError:\n",
    "            # create a new book\n",
    "            print (\"creating new {}\".format(outfile))\n",
    "            wb = xw.Book()\n",
    "            wb.save(outfile)\n",
    "\n",
    "        for key in df_dict.keys():\n",
    "            try:\n",
    "                ws = wb.sheets.add(key)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "            \n",
    "            ws = wb.sheets[key]\n",
    "\n",
    "            table_name = key\n",
    "\n",
    "            ws.clear()\n",
    "\n",
    "            df = df_dict[key].set_index(list(df_dict[key])[0])\n",
    "            if table_name in [table.df for table in ws.tables]:\n",
    "                ws.tables[table_name].update(df)\n",
    "            else:\n",
    "                table_name = ws.tables.add(source=ws['A1'],\n",
    "                                            name=table_name).update(df)\n",
    "    wb.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xl_sub_system(dict_checks):\n",
    "    sub_sys = dict_checks[check]['Sub System'].unique()\n",
    "    sub_sys.sort()\n",
    "\n",
    "    for s_sys in sub_sys:\n",
    "\n",
    "        df_temp = dict_checks[check][dict_checks[check]['Sub System'] == s_sys]\n",
    "\n",
    "        if df_temp.shape[0] > 0:\n",
    "            df_temp.to_excel(writer, sheet_name=s_sys, index=False)\n",
    "\n",
    "            ws = writer.sheets[s_sys]\n",
    "            wb = writer.book\n",
    "\n",
    "            excel_formatting.adjust_col_width_from_col(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIH_POA\n",
      "BOP_FIP\n",
      "FAS_FAS\n",
      "FIP_FIP\n",
      "FIP_FAS\n",
      "Non_MIH_AIH_Level_4\n",
      "FAS_Wrong_Parent_Source_code\n",
      "FAS_as_BOF\n",
      "make_no_buy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # for reading in multiple files\n",
    "\n",
    "    # files = find_files()\n",
    "    dict_df = {}\n",
    "\n",
    "    file = Path('/Users/mark/Downloads/Updated_T48e-01-Z00001_2024-07-19.xlsx')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    existing_bom = pd.DataFrame()\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        # reading in the historic excel files\n",
    "        df = pd.read_excel(f, parse_dates=True)\n",
    "        f.close()\n",
    "\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.rename(columns={'index':'bom_order'}, inplace=True)\n",
    "\n",
    "    # add parent source code to each row for validation checks to come\n",
    "    df = parent_source_code(df)\n",
    "\n",
    "    sc_check_list = ['AIH_POA','BOP_FIP','FAS_FAS','FIP_FIP','FIP_FAS']\n",
    "    dict_checks, df = source_code_checks(df, sc_check_list)\n",
    "\n",
    "\n",
    "    fastener_check_list = ['^washer|^bolt|^grommet']\n",
    "    dict_checks, df = fastener_checks(dict_checks, df, fastener_check_list)\n",
    "\n",
    "    df, make_no_buy = check_make_no_buy(df)\n",
    "\n",
    "    dict_checks['make_no_buy'] = df.loc[make_no_buy]\n",
    "\n",
    "    dict_checks = filter_check_columns(dict_checks)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release status checks\n",
    "For part levels >=4 groupby part number and release status and report which parts have more than 1 release status - should be unique per part number\n",
    "\n",
    "| \t|\t|\t|Function Group|\tPart Level|\tIssue Level|\trow|\n",
    "|---|---|---|--------------|--------------|------------|-------|\n",
    "|Part Number|\tSub Group|\tRelease Status|||||\t\t\t\t\n",
    "|T33-A1117X\t|A01-Structure Systems\t|REL|\tT33-BoM-XP\t|7.0\t|1.0\t|617|\n",
    "|||AWT\t|T33-BoM-XP\t|6.0\t|1.0\t|1284|\n",
    "|T33-A1475\t|A02-Panels & Closure Systems\t|AWT\t|T33-BoM-XP\t|6.0\t|1.0\t|137|\n",
    "|||REL\t|T33-BoM-XP\t|6.0\t|1.0\t|230|\n",
    "|T33-A1476X\t|A02-Panels & Closure Systems\t|AWT\t|T33-BoM-XP\t|6.0\t|1.0\t|143|\n",
    "\n",
    "\n",
    "This is written out to excel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the validation checks to excel\n",
    "\n",
    "- collated bom multi status   \n",
    "- wrong parent parts   \n",
    "- collated and cleaned bom written out with cleaned part numbers, parent parts and release status attached   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can add a list of source code combinations to check as we find out they are not valid (mainly causing a problem for IFS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out source code checks to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to existing /Users/mark/Documents/BOM_from_3DX/Updated_T48e-01-Z00001_2024-07-19_validated.xlsx\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet2'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet3'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet4'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet5'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet6'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet7'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet8'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet9'].name.get()\n",
      "Command failed:\n",
      "\t\tOSERROR: -1728\n",
      "\t\tMESSAGE: The object you are trying to access does not exist\n",
      "\t\tCOMMAND: app(pid=44565).workbooks['Updated_T48e-01-Z00001_2024-07-19_validated.xlsx'].sheets['Sheet10'].name.get()\n"
     ]
    }
   ],
   "source": [
    "# Write out to excel\n",
    "pathfile = Path(file.name).stem\n",
    "output_file = Path(sys.path[0]) / Path(pathfile + '_validated').with_suffix('.xlsx')\n",
    "\n",
    "write_to_xl(output_file, dict_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sharepoint_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxlwings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxw\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_column_letter\n\u001b[0;32m----> 4\u001b[0m make_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43msharepoint_dir\u001b[49m, project, project \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_MakeBuyQuery.xlsm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m xw\u001b[38;5;241m.\u001b[39mApp():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sharepoint_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import xlwings as xw\n",
    "from openpyxl.utils.cell import get_column_letter\n",
    "\n",
    "make_filename = os.path.join(sharepoint_dir, project, project + '_MakeBuyQuery.xlsm')\n",
    "\n",
    "with xw.App():\n",
    "    try:\n",
    "        wb = xw.Book(make_filename)\n",
    "    except:\n",
    "        wb = xw.Book()\n",
    "    \n",
    "    ws = wb.sheets[0]\n",
    "\n",
    "    startrow=0\n",
    "\n",
    "    ws.range(\"A:XX\").clear()\n",
    "\n",
    "    ws['A1'].options(pd.DataFrame, header=1, index=False).value=existing_bom\n",
    "\n",
    "    last_col_letter = get_column_letter(existing_bom.shape[1])\n",
    "\n",
    "\n",
    "    for row in make_no_buy:\n",
    "        xw.Range('A{}:{}{}'.format(row + 2, last_col_letter, row + 2)).color = (69, 165, 237)\n",
    "\n",
    "    ws.tables.add(ws.used_range, name=\"a_table\")\n",
    "\n",
    "    # autofit the columns\n",
    "    ws.autofit('c')\n",
    "    wb.save(make_filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Source Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_source_code(df):\n",
    "\n",
    "    unstacked = df.groupby(['Part Number','Issue Level','Function Group','Sub Group','Source Code']).size().unstack()\n",
    "\n",
    "    # find number of columns dynamically, as number of unique status controls the number of columns\n",
    "    expected_status_count = len(unstacked.columns) - 1\n",
    "    unstacked2 = unstacked[unstacked.isna().sum(axis=1)!=expected_status_count]\n",
    "    unstacked2\n",
    "\n",
    "\n",
    "    multi_sc = unstacked2.reset_index().fillna('')\n",
    "\n",
    "    make_sc_cols = ['AIH','MIH','MOB']\n",
    "\n",
    "    first_cols = ['Part Number', 'Issue Level', 'Function Group', 'Sub Group']\n",
    "\n",
    "    cols_to_order = first_cols + make_sc_cols\n",
    "    sc_ordered_cols = cols_to_order + (multi_sc.columns.drop(cols_to_order).tolist())\n",
    "\n",
    "    multi_sc = multi_sc[sc_ordered_cols]\n",
    "\n",
    "    return multi_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sc = multi_source_code(existing_bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "\n",
    "multi_sc_filename = os.path.join(base, '{}-Multi-Source-Codes.xlsm'.format(project))\n",
    "\n",
    "# wb = xw.Book(path)\n",
    "\n",
    "# open file if it exists\n",
    "try:\n",
    "    wb = xw.Book(multi_sc_filename)\n",
    "# otherwise create a new file    \n",
    "except:\n",
    "    wb = xw.Book()\n",
    "\n",
    "ws = wb.sheets[0]\n",
    "\n",
    "ws.clear_contents()\n",
    "\n",
    "# autofit the columns\n",
    "wb.sheets[ws].autofit('c')\n",
    "\n",
    "startrow=0\n",
    "\n",
    "ws['A1'].options(pd.DataFrame, header=1, index=False).value=multi_sc\n",
    "\n",
    "# wb.save(multi_sc_filename) - this errors - not sure it's needed with autosave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64e4acd0b8bcdde64ca4122ca150d77580571c820a6f3cf10fee72812efda0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
